{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-fill Questionnaire using Chain of Thought or Few-Shot Examples\n",
    "\n",
    "This notebook showcases the application of few-shot examples in autofilling questionnaires. It utilizes a json file (`cset_ai_harm_questionnaire.json`) to\n",
    "provide the LLM with example responses for some use-cases.\n",
    "\n",
    "By leveraging these few-shot examples, we can enable seamless completion of lengthy questionnaires, minimizing manual effort and improving overall efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_atlas_nexus.blocks.inference import (\n",
    "    RITSInferenceEngine,\n",
    "    WMLInferenceEngine,\n",
    "    OllamaInferenceEngine,\n",
    "    VLLMInferenceEngine,\n",
    ")\n",
    "from risk_atlas_nexus.blocks.inference.params import (\n",
    "    InferenceEngineCredentials,\n",
    "    RITSInferenceEngineParams,\n",
    "    WMLInferenceEngineParams,\n",
    "    OllamaInferenceEngineParams,\n",
    "    VLLMInferenceEngineParams,\n",
    ")\n",
    "\n",
    "from risk_atlas_nexus.data import load_resource\n",
    "from risk_atlas_nexus.library import RiskAtlasNexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Risk Atlas Nexus uses Large Language Models (LLMs) to infer risks dimensions. Therefore requires access to LLMs to inference or call the model.\n",
    "\n",
    "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/risk-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-20 11:39:04:645] - INFO - RiskAtlasNexus - Created RITS inference engine.\n"
     ]
    }
   ],
   "source": [
    "# inference_engine = OllamaInferenceEngine(\n",
    "#     model_name_or_path=\"granite3.2:8b\",\n",
    "#     credentials=InferenceEngineCredentials(api_url=\"http://localhost:11434\"),\n",
    "#     parameters=OllamaInferenceEngineParams(\n",
    "#         num_predict=1000, temperature=0, repeat_penalty=1, num_ctx=8192\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# inference_engine = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"ibm/granite-20b-code-instruct\",\n",
    "#     credentials={\n",
    "#         \"api_key\": \"WML_API_KEY\",\n",
    "#         \"api_url\": \"WML_API_URL\",\n",
    "#         \"project_id\": \"WML_PROJECT_ID\",\n",
    "#     },\n",
    "#     parameters=WMLInferenceEngineParams(\n",
    "#         max_new_tokens=1000, decoding_method=\"greedy\", repetition_penalty=1\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# inference_engine = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-3.1-8b-instruct\",\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=\"VLLM_API_URL\", api_key=\"VLLM_API_KEY\"\n",
    "#     ),\n",
    "#     parameters=VLLMInferenceEngineParams(max_tokens=1000, temperature=0.7),\n",
    "# )\n",
    "\n",
    "inference_engine = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-3.3-8b-instruct\",\n",
    "    credentials={\n",
    "        \"api_key\": \"cbc683b3a1a7c52d2a73008b785d2811\",\n",
    "        \"api_url\": \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com\",\n",
    "    },\n",
    "    parameters=RITSInferenceEngineParams(max_completion_tokens=1000, temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an instance of RiskAtlasNexus\n",
    "\n",
    "_Note: (Optional)_ You can specify your own directory in `RiskAtlasNexus(base_dir=<PATH>)` to utilize custom AI ontologies. If left blank, the system will use the provided AI ontologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-20 11:39:04:877] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: None\n"
     ]
    }
   ],
   "source": [
    "risk_atlas_nexus = RiskAtlasNexus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Examples for Auto-Assist Functionality\n",
    "\n",
    "The auto-assist feature utilizes few-shot examples defined in the file `risk_atlas_nexus/data/templates/cset_ai_harm_questionnaire.json` to predict the output of the ai harm questionnaire.\n",
    "\n",
    "**Customization:**\n",
    "\n",
    "To adapt this auto-assist functionality to custom risk questionnaire, users need to provide their own set of questions, example intents, and corresponding answers in a json file such as in [cset_ai_harm_questionnaire.json](https://github.com/IBM/risk-atlas-nexus/blob/main/src/risk_atlas_nexus/data/templates/cset_ai_harm_questionnaire.json). This will enable the LLM to learn from these few-shot examples and generate responses for unseen queries.\n",
    "\n",
    "**CoT Template - Zero Shot method**\n",
    "\n",
    "Each question is accompanied by corresponding examples provided as an empty list.\n",
    "\n",
    "```shell\n",
    "  [\n",
    "      {\n",
    "          \"question\": \"Did the incident occur in a domain with physical objects? 'Yes' if the AI system(s) is embedded in hardware that can interact, affect, and change with the physical objects (roads, factories, medical facilities, etc.). 'No' if the system cannot. This includes systems that inform, detect, predict, or recommend.\",\n",
    "          \"cot_examples\": [],\n",
    "          \"response_format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\n",
    "                        \"Yes\",\n",
    "                        \"No\",\n",
    "                        \"Maybe\"\n",
    "                    ]\n",
    "                },\n",
    "                \"explanation\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"answer\",\n",
    "                \"explanation\"\n",
    "            ]\n",
    "        }\n",
    "      }\n",
    "      ...\n",
    "  ]\n",
    "```\n",
    "\n",
    "**CoT Template - Few Shot method**\n",
    "\n",
    "Each question is associated with a list of examples, each containing intent, answer, and optional explanation.\n",
    "\n",
    "```shell\n",
    "  [\n",
    "      {\n",
    "          \"question\": \"Did the incident occur in a domain with physical objects? 'Yes' if the AI system(s) is embedded in hardware that can interact, affect, and change with the physical objects (roads, factories, medical facilities, etc.). 'No' if the system cannot. This includes systems that inform, detect, predict, or recommend.\",\n",
    "          \"cot_examples\": [\n",
    "            {\n",
    "                \"intent\": \"Google's Perspective API, which assigns a toxicity score to online text, seems to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases.\",\n",
    "                \"answer\": \"No\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"An Uber autonomous vehicle (AV) in autonomous mode struck and killed a pedestrian in Tempe, Arizona.\",\n",
    "                \"answer\": \"Yes\"\n",
    "            }\n",
    "            ...\n",
    "          ],\n",
    "          \"response_format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\n",
    "                        \"Yes\",\n",
    "                        \"No\",\n",
    "                        \"Maybe\"\n",
    "                    ]\n",
    "                },\n",
    "                \"explanation\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"answer\",\n",
    "                \"explanation\"\n",
    "            ]\n",
    "          }\n",
    "      }\n",
    "      ...\n",
    "  ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load AI Harm Questionnaire\n",
    "\n",
    "**Note:** The cell below loads examples of ai harm questionnaire from Risk Atlas Master. To load your custom questionnaire, create it according to the specified format and load it instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 'Q1',\n",
       " 'question': \"Did the incident occur in a domain with physical objects? 'Yes' if the AI system(s) is embedded in hardware that can interact, affect, and change with the physical objects (roads, factories, medical facilities, etc.). 'No' if the system cannot. This includes systems that inform, detect, predict, or recommend.\",\n",
       " 'cot_examples': [{'intent': \"Google's Perspective API, which assigns a toxicity score to online text, seems to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases.\",\n",
       "   'answer': 'No'},\n",
       "  {'intent': 'An Uber autonomous vehicle (AV) in autonomous mode struck and killed a pedestrian in Tempe, Arizona.',\n",
       "   'answer': 'Yes'}],\n",
       " 'response_format': {'type': 'object',\n",
       "  'properties': {'answer': {'type': 'string', 'enum': ['Yes', 'No', 'Maybe']},\n",
       "   'explanation': {'type': 'string'}},\n",
       "  'required': ['answer', 'explanation']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_harm_questionnaire = load_resource(\"cset_ai_harm_questionnaire.json\")\n",
    "\n",
    "ai_harm_questionnaire[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use the inference engine to get the LLM outputs. `generate_zero_shot_risk_questionnaire_output` which gives the zero-shot output for the question and `generate_few_shot_risk_questionnaire_output` which gives the output using few-shot examples defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Inferring with RITS: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: Did the incident occur in a domain with physical objects? 'Yes' if the AI system(s) is embedded in hardware that can interact, affect, and change with the physical objects (roads, factories, medical facilities, etc.). 'No' if the system cannot. This includes systems that inform, detect, predict, or recommend.\n",
      "A: {'answer': 'No', 'explanation': \"The task described involves using AI to draft a government policy report, which is a domain of information and analysis rather than direct interaction with physical objects. The AI system is being used to analyze input documents and understand court rulings on automated penalties in Australia's welfare system. This process does not involve physical interaction with objects like roads, factories, or medical facilities. Instead, it focuses on processing and interpreting data from legal documents.\"}\n",
      "\n",
      "Q2: Did the AI incident occur in the entertainment industry? 'Yes' if the sector in which the AI was used is associated with entertainment. 'No' if it was used in a different, clearly identifiable sector. 'Maybe' if the sector of use could not be determined.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes the use of AI for drafting a government policy report, specifically analyzing court decisions related to automated penalties in Australia's welfare system. This task falls under the government and legal sectors, not the entertainment industry. Therefore, the AI incident did not occur in the entertainment industry.\"}\n",
      "\n",
      "Q3: Was the incident about a report, test, or study of data instead of the AI itself? 'Yes' if the incident is about a report, test, or study of the data and does not discuss an instance of injury, damage, or loss. 'Maybe' if it is unclear. Otherwise mark 'No.'\n",
      "A: {'answer': 'Yes', 'explanation': \"The intent describes a task involving the use of AI to draft a government policy report. This task focuses on analyzing input documents and understanding how Australian federal courts have addressed automated penalties in welfare systems. The description does not mention any injury, damage, or loss, but rather a study or test of data (the court decisions and automated decision-making processes) to inform a report. Therefore, the answer is 'Yes'.\"}\n",
      "\n",
      "Q4: Was the reported system (even if AI involvement is unknown) deployed or sold to users? 'Yes' if the involved system was deployed or sold to users. 'No' if it was not. 'Maybe' if there is not enough information or if the use is unclear.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes a task involving the use of AI to draft a government policy report. This does not imply that any specific AI system has been deployed or sold to users for automated decision-making in Australia's welfare system. Instead, it refers to using AI as a tool to assist in the research and drafting process of the report.\"}\n",
      "\n",
      "Q5: Was this a test or demonstration of an AI system done by developers, producers or researchers (versus users) in controlled conditions? 'Yes' if it was a test/demonstration performed by developers, producers or researchers in controlled conditions. 'No' if it was not a test/demonstration. 'No' if the test/demonstration was done by a user. 'No' if the test/demonstration was in operational or uncontrolled conditions. 'Maybe' otherwise.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes a task assigned to an AI system for drafting a comprehensive section of a government policy report. This task involves analyzing input documents to understand how Australian federal courts have addressed automated penalties in welfare systems. The task is being performed for a specific purpose (writing a policy report) and not as a test or demonstration by developers, producers, or researchers in controlled conditions. Therefore, the answer is 'No'.\"}\n",
      "\n",
      "Q6: Was this a test or demonstration of an AI system done by developers, producers or researchers (versus users) in operational conditions? 'Yes' if it was a test/demonstration performed by developers, producers or researchers in controlled conditions. 'No' if it was not a test/demonstration. 'No' if the test/demonstration was done by a user. 'No' if the test/demonstration was in controlled or non-operational conditions. 'Maybe' otherwise.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes a task assigned to an AI system for drafting a comprehensive section of a government policy report. This task involves analyzing input documents to understand how Australian federal courts have addressed automated penalties in welfare systems and ensuring the output is factually correct. This scenario does not describe a test or demonstration performed by developers, producers, or researchers in controlled conditions, nor was it executed by a user in operational conditions. Instead, it's an application of AI for a specific purpose, making the answer 'No'.\"}\n",
      "\n",
      "Q7: Was this a test or demonstration done by users in controlled conditions? 'Yes' if it was a test/demonstration performed by users in controlled conditions. 'No' if it was not a test demonstration. 'No' if the test/demonstration was done by developers, producers or researchers. 'No' if the test demonstration was in operational or uncontrolled conditions.'Maybe' otherwise.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes a task of using AI to draft a government policy report on automated decision-making in Australia's welfare system. This task involves analyzing input documents to understand how Australian federal courts have addressed automated penalties in welfare systems. There is no mention of a test or demonstration performed by users in controlled conditions, nor is it indicated that the task was carried out by developers, producers, or researchers under controlled conditions. Therefore, the answer is 'No'.\"}\n",
      "\n",
      "Q8: Was this a test or demonstration done by users in operational conditions? 'Yes' if it was a test/demonstration performed by users in operational conditions. 'No' if it was not a test demonstration. 'No' if the test/demonstration was done by developers, producers or researchers. 'No' if the test demonstration was in controlled or non-operational conditions.'Maybe' otherwise.\n",
      "A: {'answer': 'No', 'explanation': \"This scenario describes a task assigned to an AI for drafting a government policy report, which is not a test or demonstration performed by users in operational conditions. The AI is being used by developers or researchers for a specific project, not by end-users in a live environment. Therefore, the appropriate response is 'No'.\"}\n",
      "\n",
      "Q9: Reflecting upon the previously answered questions, decide if the reported incident occurred in a domain in which harm could possibly occur. This is not a decision on whether or not harm did occur.\n",
      "A: {'answer': 'Yes', 'explanation': \"The reported incident involves the use of AI in a sensitive domain, specifically in the context of automated decision-making within Australia's welfare system. Automated decision-making systems can potentially cause harm if they lead to incorrect or unfair decisions, such as erroneous denials of welfare benefits. This could result in significant hardship for individuals and families relying on these benefits. Therefore, it is crucial to thoroughly analyze the input documents and understand how Australian federal courts have addressed automated penalties in this context to ensure the policy report is factually correct and mitigates potential harm.\"}\n",
      "\n",
      "Q10: Does the incident involve an AI system? Checks if the technology involved is a simple automation technology or a rule-based software system, or if it is based on machine-learning.\n",
      "A: {'answer': 'Yes', 'explanation': 'The task involves using AI to draft a comprehensive section of a government policy report. Specifically, the AI will analyze input documents to understand how Australian federal courts have addressed automated penalties in welfare systems. This process requires the AI to process, comprehend, and extract information from legal documents, which falls under the category of machine-learning applications.'}\n",
      "\n",
      "Q11: If the behavior of the technology described in the incident can be directly and clearly linked to the adverse or potentially adverse outcome described in the incident. ? 'Yes' if the technology was involved in harm, its behavior can be directly linked to the harm, and the harm may not have occurred if the technology acted differently. 'Maybe' if the link is unclear. Otherwise, select 'No.'.\n",
      "A: {'answer': 'No', 'explanation': \"The intent describes a task of using AI to draft a government policy report on automated decision-making in Australia's welfare system. It does not mention any adverse or potentially adverse outcomes caused by the technology. Instead, it focuses on the AI's role in analyzing input documents and drafting a comprehensive section of the report. There is no direct link between the behavior of the technology and any harm in this scenario.\"}\n",
      "\n",
      "Q12: Is there a characterizable class or subgroup of entities that experienced the harm? 'Yes' if the harmed entity or entities can be characterized. 'No' if there are not any characterizable entities.\n",
      "A: {'answer': 'No', 'explanation': \"The intent does not describe any harm inflicted on a characterizable class or subgroup of entities. Instead, it outlines a task to draft a policy report on automated decision-making in Australia's welfare system, focusing on court decisions regarding automated penalties. There is no mention of harm or affected entities in this context.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "usecase = \"Using AI to  write a government policy report on automated decision-making in Australia's welfare system. The task will include  drafting a comprehensive section analyzing input documents to understand how Australian federal courts have addressed automated penalties in welfare systems and ensure the output is factually correct.\"\n",
    "\n",
    "results = risk_atlas_nexus.generate_few_shot_risk_questionnaire_output(\n",
    "    usecase,\n",
    "    ai_harm_questionnaire,\n",
    "    inference_engine,\n",
    ")\n",
    "\n",
    "for index, (risk_question, result) in enumerate(\n",
    "    zip(ai_harm_questionnaire, results), start=1\n",
    "):\n",
    "    print(f\"\\nQ{index}: \" + risk_question[\"question\"] + \"\\nA: \" + str(result.prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
